{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4 (Part II)\n",
    "\n",
    "Aprendizagem 2023/2024 - LEIC @ IST \n",
    "\n",
    "Group #24\n",
    "- Daniel Nunes (N¬∫ 103095)\n",
    "- Gon√ßalo Alves (N¬∫ 103540)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data importing and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.35568788, 0.51989984, 0.22917997, 0.2508573 , 0.30746116,\n",
       "        0.02514839],\n",
       "       [0.12450104, 0.2967831 , 0.09857833, 0.14462935, 0.47664891,\n",
       "        0.03636497],\n",
       "       [0.41166648, 0.51393229, 0.32299466, 0.30766054, 0.38609692,\n",
       "        0.0175229 ],\n",
       "       ...,\n",
       "       [0.34043781, 0.52244298, 0.28789745, 0.23490726, 0.59779618,\n",
       "        0.01943732],\n",
       "       [0.18425678, 0.27235174, 0.24684569, 0.21462279, 0.52117504,\n",
       "        0.02624045],\n",
       "       [0.07420202, 0.20770855, 0.20261992, 0.14251659, 0.57924032,\n",
       "        0.02527676]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.io import arff\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load your ARFF file\n",
    "data, meta = arff.loadarff('column_diagnosis.arff')\n",
    "\n",
    "# Convert the ARFF data to a NumPy array\n",
    "data = np.array(data.tolist())\n",
    "\n",
    "# Extract the numeric features (the first 6 columns) for normalization\n",
    "numeric_data = data[:, :-1].astype(float)\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data using the scaler\n",
    "normalized_data = scaler.fit_transform(numeric_data)\n",
    "\n",
    "display(normalized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Using sklearn, apply k-means clustering fully unsupervisedly on the normalized data with \n",
    "ùëò ‚àà{2,3,4,5} (random=0 and remaining parameters as default). Assess the silhouette and purity of \n",
    "the produced solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette score for k=2: 0.20730150522479207\n",
      "Accuracy (Purity) for k=2: 0.02258064516129032\n",
      "Silhouette score for k=3: 0.20730150522479207\n",
      "Accuracy (Purity) for k=3: 0.02258064516129032\n",
      "Silhouette score for k=4: 0.20730150522479207\n",
      "Accuracy (Purity) for k=4: 0.02258064516129032\n",
      "Silhouette score for k=5: 0.20730150522479207\n",
      "Accuracy (Purity) for k=5: 0.02258064516129032\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Define the range of k values to try\n",
    "k_values = [2, 3, 4, 5]\n",
    "\n",
    "# Perform k-means clustering for different values of k\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=8, random_state=0, n_init='auto') # the default value of n_clusters is 8 \n",
    "    cluster_labels = kmeans.fit_predict(normalized_data)\n",
    "    \n",
    "    # Calculate silhouette score\n",
    "    silhouette_avg = silhouette_score(normalized_data, cluster_labels)\n",
    "    print(f\"Silhouette score for k={k}: {silhouette_avg}\")\n",
    "    \n",
    "    # Assess purity\n",
    "    # You need to know the ground truth labels to calculate purity\n",
    "    # If you don't have ground truth labels, purity cannot be calculated\n",
    "    ground_truth_labels = data[:, -1].astype(str)\n",
    "    le = LabelEncoder()\n",
    "    ground_truth_labels_encoded = le.fit_transform(ground_truth_labels)\n",
    "    \n",
    "    # Assuming cluster_labels and ground_truth_labels_encoded have the same length\n",
    "    accuracy = accuracy_score(ground_truth_labels_encoded, cluster_labels)\n",
    "    print(f\"Accuracy (Purity) for k={k}: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Consider the application of PCA after the data normalization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. Identify the variability explained by the top two principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The variability of the top two principal components \n",
      "is equal to [0.56181445 0.20955953]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(normalized_data)\n",
    "\n",
    "explained_var_ratio = pca.explained_variance_ratio_\n",
    "print(f\"The variability of the top two principal components \\n\" +\n",
    "      \"is equal to\", explained_var_ratio)\n",
    "\n",
    "# TODO: Verify this exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii. For each one of these two components, sort the input variables by relevance by inspecting the absolute weights of the linear projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Visualize side-by-side the data using: i) the ground diagnoses, and ii) the previously learned k=3 clustering solution. To this end, projected the normalized data onto a 2-dimensional data space using PCA and then color observations using the reference and cluster annotations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "Considering the results from questions (1) and (3), identify two ways on how clustering can be used to characterize the population of ill and healthy individuals. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our answer**\n",
    "\n",
    "We can conclude from this data that, as the number of maximum iterations for training a model increases, its RMSE values decrease, meaning that the model gets more accurate as the amount of training increases.\n",
    "\n",
    "However, too many iterations may start to cause overfitting on our model, meaning that, while it follows its predictions closer to the training data, it can be more error-prone when evaluating completely new sets of data. Hence, the early-stopping strategy tries to strike a balance between the model's accuracy to the training data and the possibility of overfitting, favouring performance in most cases. This might be the reason why its RMSE value is slightly larger than the one calculated from the model that stops training after 200 iterations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
